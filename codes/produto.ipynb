{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.45.1-py3-none-any.whl.metadata (44 kB)\n",
      "Collecting torch\n",
      "  Downloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl.metadata (26 kB)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (13 kB)\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.1.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting trafilatura\n",
      "  Downloading trafilatura-1.12.2-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.23.2 (from transformers)\n",
      "  Downloading huggingface_hub-0.25.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Collecting pyyaml>=5.1 (from transformers)\n",
      "  Using cached PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tokenizers<0.21,>=0.20 (from transformers)\n",
      "  Downloading tokenizers-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting tqdm>=4.27 (from transformers)\n",
      "  Downloading tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions>=4.8.0 (from torch)\n",
      "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting sympy (from torch)\n",
      "  Downloading sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.3-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting setuptools (from torch)\n",
      "  Using cached setuptools-75.1.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
      "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
      "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
      "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
      "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
      "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
      "  Downloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
      "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting triton==3.0.0 (from torch)\n",
      "  Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting click (from sacremoses)\n",
      "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: certifi in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from trafilatura) (2024.8.30)\n",
      "Collecting courlan>=1.2.0 (from trafilatura)\n",
      "  Downloading courlan-1.3.1-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting htmldate>=1.8.1 (from trafilatura)\n",
      "  Downloading htmldate-1.9.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting justext>=3.0.1 (from trafilatura)\n",
      "  Downloading jusText-3.0.1-py2.py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting lxml>=5.2.2 (from trafilatura)\n",
      "  Downloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: charset-normalizer>=3.2.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from trafilatura) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from trafilatura) (2.2.3)\n",
      "Collecting babel>=2.16.0 (from courlan>=1.2.0->trafilatura)\n",
      "  Using cached babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting tld>=0.13 (from courlan>=1.2.0->trafilatura)\n",
      "  Downloading tld-0.13-py2.py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting dateparser>=1.1.2 (from htmldate>=1.8.1->trafilatura)\n",
      "  Downloading dateparser-1.2.0-py2.py3-none-any.whl.metadata (28 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from htmldate>=1.8.1->trafilatura) (2.9.0.post0)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Using cached MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: pytz in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from dateparser>=1.1.2->htmldate>=1.8.1->trafilatura) (2024.2)\n",
      "Collecting tzlocal (from dateparser>=1.1.2->htmldate>=1.8.1->trafilatura)\n",
      "  Downloading tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting lxml-html-clean (from lxml[html_clean]>=4.4.2->justext>=3.0.1->trafilatura)\n",
      "  Downloading lxml_html_clean-0.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->htmldate>=1.8.1->trafilatura) (1.16.0)\n",
      "Downloading transformers-4.45.1-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m66.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.4.1-cp312-cp312-manylinux1_x86_64.whl (797.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 MB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m36.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m68.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
      "Downloading triton-3.0.0-1-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m49.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.5.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.9/12.9 MB\u001b[0m \u001b[31m59.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading sentencepiece-0.2.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading trafilatura-1.12.2-py3-none-any.whl (132 kB)\n",
      "Downloading courlan-1.3.1-py3-none-any.whl (33 kB)\n",
      "Downloading htmldate-1.9.0-py3-none-any.whl (31 kB)\n",
      "Downloading huggingface_hub-0.25.1-py3-none-any.whl (436 kB)\n",
      "Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading jusText-3.0.1-py2.py3-none-any.whl (837 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m837.8/837.8 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading lxml-5.3.0-cp312-cp312-manylinux_2_28_x86_64.whl (4.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.9/4.9 MB\u001b[0m \u001b[31m57.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyYAML-6.0.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (767 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m767.5/767.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.9.11-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (797 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m797.0/797.0 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (434 kB)\n",
      "Downloading scipy-1.14.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (40.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 MB\u001b[0m \u001b[31m63.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Downloading tokenizers-0.20.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m37.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Downloading click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.3-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached setuptools-75.1.0-py3-none-any.whl (1.2 MB)\n",
      "Downloading sympy-1.13.3-py3-none-any.whl (6.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.2/6.2 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dateparser-1.2.0-py2.py3-none-any.whl (294 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tld-0.13-py2.py3-none-any.whl (263 kB)\n",
      "Downloading nvidia_nvjitlink_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (19.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m76.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading lxml_html_clean-0.2.2-py3-none-any.whl (13 kB)\n",
      "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: sentencepiece, mpmath, tzlocal, typing-extensions, tqdm, tld, threadpoolctl, sympy, setuptools, scipy, safetensors, regex, pyyaml, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, networkx, MarkupSafe, lxml, joblib, fsspec, filelock, click, babel, triton, scikit-learn, sacremoses, nvidia-cusparse-cu12, nvidia-cudnn-cu12, lxml-html-clean, jinja2, huggingface-hub, dateparser, courlan, tokenizers, nvidia-cusolver-cu12, htmldate, transformers, torch, justext, trafilatura\n",
      "Successfully installed MarkupSafe-2.1.5 babel-2.16.0 click-8.1.7 courlan-1.3.1 dateparser-1.2.0 filelock-3.16.1 fsspec-2024.9.0 htmldate-1.9.0 huggingface-hub-0.25.1 jinja2-3.1.4 joblib-1.4.2 justext-3.0.1 lxml-5.3.0 lxml-html-clean-0.2.2 mpmath-1.3.0 networkx-3.3 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.6.77 nvidia-nvtx-cu12-12.1.105 pyyaml-6.0.2 regex-2024.9.11 sacremoses-0.1.1 safetensors-0.4.5 scikit-learn-1.5.2 scipy-1.14.1 sentencepiece-0.2.0 setuptools-75.1.0 sympy-1.13.3 threadpoolctl-3.5.0 tld-0.13 tokenizers-0.20.0 torch-2.4.1 tqdm-4.66.5 trafilatura-1.12.2 transformers-4.45.1 triton-3.0.0 typing-extensions-4.12.2 tzlocal-5.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch scikit-learn sentencepiece sacremoses trafilatura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (1.51.0)\n",
      "Requirement already satisfied: pandas in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: openpyxl in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (3.1.5)\n",
      "Collecting google-generativeai\n",
      "  Downloading google_generativeai-0.8.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from openai) (4.6.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from openai) (0.27.2)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from openai) (0.5.0)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from openai) (2.9.2)\n",
      "Requirement already satisfied: sniffio in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from openai) (1.3.1)\n",
      "Requirement already satisfied: tqdm>4 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from openai) (4.12.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: et-xmlfile in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from openpyxl) (1.1.0)\n",
      "Collecting google-ai-generativelanguage==0.6.10 (from google-generativeai)\n",
      "  Using cached google_ai_generativelanguage-0.6.10-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting google-api-core (from google-generativeai)\n",
      "  Downloading google_api_core-2.21.0-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting google-api-python-client (from google-generativeai)\n",
      "  Downloading google_api_python_client-2.149.0-py2.py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting google-auth>=2.15.0 (from google-generativeai)\n",
      "  Using cached google_auth-2.35.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting protobuf (from google-generativeai)\n",
      "  Using cached protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting proto-plus<2.0.0dev,>=1.22.3 (from google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Using cached proto_plus-1.24.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: idna>=2.8 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
      "Collecting googleapis-common-protos<2.0.dev0,>=1.56.2 (from google-api-core->google-generativeai)\n",
      "  Using cached googleapis_common_protos-1.65.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from google-api-core->google-generativeai) (2.32.3)\n",
      "Collecting cachetools<6.0,>=2.0.0 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached cachetools-5.5.0-py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1_modules-0.4.1-py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: certifi in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.6)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.23.4)\n",
      "Requirement already satisfied: six>=1.5 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Collecting httplib2<1.dev0,>=0.19.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting google-auth-httplib2<1.0.0,>=0.2.0 (from google-api-python-client->google-generativeai)\n",
      "  Using cached google_auth_httplib2-0.2.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting uritemplate<5,>=3.0.1 (from google-api-python-client->google-generativeai)\n",
      "  Downloading uritemplate-4.1.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting grpcio<2.0dev,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Using cached grpcio-1.66.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting grpcio-status<2.0.dev0,>=1.33.2 (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.1->google-ai-generativelanguage==0.6.10->google-generativeai)\n",
      "  Using cached grpcio_status-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 (from httplib2<1.dev0,>=0.19.0->google-api-python-client->google-generativeai)\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth>=2.15.0->google-generativeai)\n",
      "  Using cached pyasn1-0.6.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google-generativeai) (2.2.3)\n",
      "Downloading google_generativeai-0.8.3-py3-none-any.whl (160 kB)\n",
      "Downloading google_ai_generativelanguage-0.6.10-py3-none-any.whl (760 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.0/760.0 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading google_api_core-2.21.0-py3-none-any.whl (156 kB)\n",
      "Downloading google_auth-2.35.0-py2.py3-none-any.whl (208 kB)\n",
      "Downloading protobuf-5.28.2-cp38-abi3-manylinux2014_x86_64.whl (316 kB)\n",
      "Downloading google_api_python_client-2.149.0-py2.py3-none-any.whl (12.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m75.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading cachetools-5.5.0-py3-none-any.whl (9.5 kB)\n",
      "Downloading google_auth_httplib2-0.2.0-py2.py3-none-any.whl (9.3 kB)\n",
      "Downloading googleapis_common_protos-1.65.0-py2.py3-none-any.whl (220 kB)\n",
      "Downloading httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Downloading proto_plus-1.24.0-py3-none-any.whl (50 kB)\n",
      "Downloading pyasn1_modules-0.4.1-py3-none-any.whl (181 kB)\n",
      "Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Downloading uritemplate-4.1.1-py2.py3-none-any.whl (10 kB)\n",
      "Downloading grpcio-1.66.2-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio_status-1.66.2-py3-none-any.whl (14 kB)\n",
      "Downloading pyasn1-0.6.1-py3-none-any.whl (83 kB)\n",
      "Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "Installing collected packages: uritemplate, pyparsing, pyasn1, protobuf, grpcio, cachetools, rsa, pyasn1-modules, proto-plus, httplib2, googleapis-common-protos, grpcio-status, google-auth, google-auth-httplib2, google-api-core, google-api-python-client, google-ai-generativelanguage, google-generativeai\n",
      "Successfully installed cachetools-5.5.0 google-ai-generativelanguage-0.6.10 google-api-core-2.21.0 google-api-python-client-2.149.0 google-auth-2.35.0 google-auth-httplib2-0.2.0 google-generativeai-0.8.3 googleapis-common-protos-1.65.0 grpcio-1.66.2 grpcio-status-1.66.2 httplib2-0.22.0 proto-plus-1.24.0 protobuf-5.28.2 pyasn1-0.6.1 pyasn1-modules-0.4.1 pyparsing-3.1.4 rsa-4.9 uritemplate-4.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install openai pandas openpyxl google-generativeai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code - 1 - webscraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import trafilatura\n",
    "\n",
    "\n",
    "# Função para extrair o conteúdo de uma página web usando Trafilatura\n",
    "def extrair_texto(url):\n",
    "    downloaded = trafilatura.fetch_url(url)  # Baixar o conteúdo da página\n",
    "    if downloaded:\n",
    "        return trafilatura.extract(downloaded)  # Extrair o texto da página\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Ler o arquivo Excel com links das notícias\n",
    "df = pd.read_excel('./database/finance_news.xlsx')  # Substitua pelo nome correto do seu arquivo .xlsx\n",
    "\n",
    "# Criar uma lista para armazenar textos e títulos\n",
    "lista_textos = []\n",
    "lista_titulos = []\n",
    "\n",
    "# Iterar por cada link e extrair o conteúdo\n",
    "for index, row in df.iterrows():\n",
    "    url = row['link']  # Substitua 'link' pelo nome correto da coluna que contém os URLs\n",
    "    texto = extrair_texto(url)\n",
    "    \n",
    "    if texto:\n",
    "        lista_textos.append(texto)  # Adicionar o texto extraído à lista\n",
    "        lista_titulos.append(row['title'])  # Adicionar o título à lista\n",
    "    else:\n",
    "        lista_textos.append('Texto não encontrado')\n",
    "        lista_titulos.append('Título não encontrado')\n",
    "\n",
    "# Criar um novo DataFrame com os títulos\n",
    "df_produto = pd.DataFrame({\n",
    "    'titulos': lista_titulos\n",
    "})\n",
    "\n",
    "# Salvar o novo DataFrame em um arquivo Excel chamado Produto.xlsx\n",
    "df_produto.to_excel('./database/Produto.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code - 2 - Resumos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configurar a API do Gemini\n",
    "API_KEY = 'AIzaSyD-EeX6oXIDPhXjAnBfTEsjXCciiB8ifgc'\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# Inicializar a lista de resumos identificados\n",
    "todos_resumos = []\n",
    "\n",
    "# Loop para utilizar cada texto extraído e fazer a solicitação à API Gemini\n",
    "for texto in lista_textos:\n",
    "    try:\n",
    "        # Fazer a chamada à API Gemini com o texto extraído\n",
    "        response = model.generate_content(f\"\"\"\n",
    "                                       {texto}\n",
    "                                        faça resumo dessa noticia com 820 caracteres\n",
    "                                       \"\"\")\n",
    "        \n",
    "        # Capturar o resultado (resumos) e adicionar à lista geral\n",
    "        resumos = response.text  # Captura o texto gerado pela API\n",
    "        todos_resumos.append(resumos)  # Adiciona o resultado à lista de resumos\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o texto: {e}\")\n",
    "        todos_resumos.append('Erro ao processar')\n",
    "\n",
    "# Ler o arquivo Produto.xlsx já criado\n",
    "df_produto = pd.read_excel('./database/Produto.xlsx')\n",
    "\n",
    "# Adicionar a nova coluna com os tickers ao DataFrame existente\n",
    "df_produto['Notícias'] = todos_resumos\n",
    "\n",
    "# Salvar o DataFrame atualizado com os tickers no mesmo arquivo Produto.xlsx\n",
    "df_produto.to_excel('./database/Produto.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code - 3 - Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n",
      "Erro ao processar o texto: 429 Resource has been exhausted (e.g. check quota).\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "# Configurar a API do Gemini\n",
    "API_KEY = 'AIzaSyD-EeX6oXIDPhXjAnBfTEsjXCciiB8ifgc'\n",
    "model = genai.GenerativeModel(\"gemini-1.5-pro\")\n",
    "genai.configure(api_key=API_KEY)\n",
    "\n",
    "# Inicializar a lista de tickers identificados\n",
    "todos_tickers = []\n",
    "\n",
    "# Loop para utilizar cada texto extraído e fazer a solicitação à API Gemini\n",
    "for texto in lista_textos:\n",
    "    try:\n",
    "        # Fazer a chamada à API Gemini com o texto extraído\n",
    "        response = model.generate_content(f\"\"\"\n",
    "                                       {texto}\n",
    "                                       fale os tickers financeiros identificados nesse texto com esse prompt:\n",
    "                                       nome_da_empresa(ticker)\n",
    "                                       \"\"\")\n",
    "        \n",
    "        # Capturar o resultado (tickers) e adicionar à lista geral\n",
    "        tickers = response.text  # Captura o texto gerado pela API\n",
    "        todos_tickers.append(tickers)  # Adiciona o resultado à lista de tickers\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Erro ao processar o texto: {e}\")\n",
    "        todos_tickers.append('Erro ao processar')\n",
    "\n",
    "# Ler o arquivo Produto.xlsx já criado\n",
    "df_produto = pd.read_excel('./database/Produto.xlsx')\n",
    "\n",
    "# Adicionar a nova coluna com os tickers ao DataFrame existente\n",
    "df_produto['tickers'] = todos_tickers\n",
    "\n",
    "# Salvar o DataFrame atualizado com os tickers no mesmo arquivo Produto.xlsx\n",
    "df_produto.to_excel('./database/Produto.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code - 4 - sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/SynaptaInvest/.venv/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from transformers import MarianMTModel, MarianTokenizer\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# Carregar o modelo e tokenizer do FinBERT\n",
    "tokenizer = BertTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = BertForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "\n",
    "# Carregar o modelo de tradução multilíngue MarianMT (pt -> en)\n",
    "modelo_traducao = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-mul-en')\n",
    "tokenizer_traducao = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-mul-en')\n",
    "\n",
    "# Função para traduzir de português para inglês\n",
    "def traduzir(texto_pt):\n",
    "    tokens = tokenizer_traducao(texto_pt, return_tensors=\"pt\", padding=True)\n",
    "    traducao = modelo_traducao.generate(**tokens)\n",
    "    texto_traduzido = tokenizer_traducao.decode(traducao[0], skip_special_tokens=True)\n",
    "    return texto_traduzido\n",
    "\n",
    "# Função para realizar análise de sentimento diretamente com o FinBERT e obter as probabilidades\n",
    "def analisar_sentimento(texto):\n",
    "    inputs = tokenizer(texto, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    # Aplicar softmax para obter as probabilidades\n",
    "    probabilidades = F.softmax(logits, dim=1).detach().cpu().numpy()[0]\n",
    "    \n",
    "    # O FinBERT foi treinado para 3 classes: 0 (negativo), 1 (neutro), 2 (positivo)\n",
    "    sentimentos = {\n",
    "        \"positive\": round(probabilidades[2] * 100, 2),  # Reduzido para 2 casas decimais\n",
    "        \"neutral\": round(probabilidades[1] * 100, 2),   # Reduzido para 2 casas decimais\n",
    "        \"negative\": round(probabilidades[0] * 100, 2)   # Reduzido para 2 casas decimais\n",
    "    }\n",
    "    return sentimentos\n",
    "\n",
    "# Caminho para o arquivo de saída com os resumos já gerados\n",
    "file_path = './database/Produto.xlsx'\n",
    "df_produto = pd.read_excel(file_path)\n",
    "\n",
    "# Lista para armazenar o maior sentimento de cada notícia\n",
    "maiores_sentimentos = []\n",
    "\n",
    "# Loop para analisar cada resumo na coluna \"resumo\"\n",
    "for resumo in df_produto['Notícias']:\n",
    "    # Traduzir o resumo para inglês\n",
    "    resumo_traduzido = traduzir(resumo)\n",
    "\n",
    "    # Realizar a análise de sentimento no resumo traduzido\n",
    "    sentimentos = analisar_sentimento(resumo_traduzido)\n",
    "\n",
    "    # Determinar o sentimento com a maior porcentagem\n",
    "    sentimento_maior = max(sentimentos, key=sentimentos.get)\n",
    "    porcentagem_maior = round(sentimentos[sentimento_maior], 2)  # Garantir 2 casas decimais\n",
    "\n",
    "    # Adicionar o sentimento e a porcentagem correspondente na lista\n",
    "    maiores_sentimentos.append(f\"{sentimento_maior} ({porcentagem_maior}%)\")\n",
    "\n",
    "# Adicionar a nova coluna \"sentimentos\" ao DataFrame com os maiores sentimentos\n",
    "df_produto['Sentimentos'] = maiores_sentimentos\n",
    "\n",
    "# Salvar o DataFrame atualizado de volta no arquivo Excel\n",
    "df_produto.to_excel(file_path, index=False)\n",
    "\n",
    "# Exibir uma mensagem de sucesso\n",
    "print(f\"Arquivo 'Produto.xlsx' atualizado com sucesso!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code - 5 - Relatorio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo 'produto.xlsx' atualizado com a coluna 'relatorio' com sucesso!\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
